{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Zhihao Zhang\n",
    "# NetID: zz2432\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My impelmentation of logistic regression on lasso regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The idea of how to implement lasso comes from this post: https://xavierbourretsicotte.github.io/lasso_derivation.html\n",
    "\n",
    "Since the lasso term is indifferentiable at 0, we need to look at different intervals and find the the relative changes, which is usage of subderivative.\n",
    "\n",
    "\n",
    "<img src=\"equation1.png\" height=\"500\" width=\"500\">\n",
    "\n",
    "Then set three cases to zero, we can compute three equations under different condition, which update the current jth feature of X\n",
    "\n",
    "We create a soft_threhold function to evaluate those three conditions\n",
    "\n",
    "\n",
    "<img src=\"equation_formula_2.png\" height=\"600\" width=\"600\">\n",
    "<img src=\"equation3.png\" height=\"600\" width=\"600\">\n",
    "\n",
    "During the updating stage, we update weights feature-wise. For example, theta= [theta1, theta2, theta3]. Then we look at all the examples for theta1, follow the idea about to compute the gradient and use solf-threhold to evaluate it then update theta1; Next turn is theta2 for all the examples, and follow the same procedures until we finished updating all thetas.\n",
    "\n",
    "Repeat this process for number of iterations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1+np.e**(-z))\n",
    "\n",
    "def hypothesis(X , w):\n",
    "    return sigmoid(X.dot(w))\n",
    "\n",
    " \n",
    "def soft_threshold(rho,lamda):\n",
    "    '''Soft threshold function used for normalized data and lasso regression'''\n",
    "    if rho < - lamda:\n",
    "        return (rho + lamda)\n",
    "    elif rho >  lamda:\n",
    "        return (rho - lamda)\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "def coordinate_descent_lasso(X, y, learning_rate, num_iters, lamda = 0.01):\n",
    "    '''Coordinate gradient descent for lasso regression'''\n",
    "    # Initialize w to be a zero vector\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    #Initialisation of useful values \n",
    "    m,n = X.shape\n",
    "    # reshape y as a column vector\n",
    "    y = y.reshape(y.shape[0],1)\n",
    "\n",
    "    for i in range(num_iters):       \n",
    "        #Looping through each coordinate, update each feature accordingly\n",
    "        for j in range(n):      \n",
    "            #Vectorized implementation\n",
    "            X_j = X[:,j].reshape(m,1)\n",
    "            y_pred = hypothesis(X,w)\n",
    "            rho = (learning_rate/m) * X_j.T.dot(y - y_pred  + w[j]*X_j)\n",
    " \n",
    "            if j == 0: \n",
    "                w[j] =  rho \n",
    "            else:\n",
    "                w[j] =  soft_threshold(rho, lamda)  \n",
    "   \n",
    "            \n",
    "    return w.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset used in class: breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "X_train_tmp, X_test_tmp, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# Scale the data since we will be using gradient ascent\n",
    "scaler = preprocessing.StandardScaler().fit(X_train_tmp)\n",
    "X_train = scaler.transform(X_train_tmp)\n",
    "X_test = scaler.transform(X_test_tmp)\n",
    "\n",
    "# Append a column of ones to X_train and X_test\n",
    "ones = np.ones(X_train.shape[0]).reshape((X_train.shape[0],1))\n",
    "X_train = np.hstack((ones, X_train))\n",
    "ones = np.ones(X_test.shape[0]).reshape((X_test.shape[0], 1))\n",
    "X_test = np.hstack((ones, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "[ 0.27492043 -0.2097498  -0.13611075 -0.21051871 -0.19045538 -0.03009046\n",
      " -0.00855919 -0.15174555 -0.27884892  0.          0.         -0.14630241\n",
      "  0.         -0.10525505 -0.09686716  0.          0.          0.\n",
      "  0.          0.          0.00114735 -0.27301691 -0.22092445 -0.26407832\n",
      " -0.22755563 -0.15351341 -0.10787207 -0.18899421 -0.2921711  -0.17405755\n",
      "  0.        ]\n",
      "Accuracy: 0.9787234042553191\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.9\n",
    "new_w = coordinate_descent_lasso(X_train, y_train, learning_rate,num_iters=5000, lamda=0.02)\n",
    "print(f'Weights:\\n{new_w}')\n",
    "# set threshold\n",
    "threshold = 0.5\n",
    "y_pred = hypothesis(X_test,new_w)\n",
    "ones_idx,zeros_idx = np.where(y_pred >=threshold), np.where(y_pred <threshold)\n",
    "y_pred[ones_idx],y_pred[zeros_idx] = 1, 0\n",
    "# change y_pred shape to the same shape as y_pred\n",
    "y_pred = y_pred.reshape(y_pred.shape[0])\n",
    "print('Accuracy: {}'.format(np.where(y_pred == y_test)[0].size / y_pred.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset outside class: wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_0' 'class_1' 'class_2']\n",
      "we only focus on class_0 and class_1 for the purpose of simplicity. (do binary classification)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "data = load_wine()\n",
    "target_name = data.target_names\n",
    "print(target_name)\n",
    "print('we only focus on class_0 and class_1 for the purpose of simplicity. (do binary classification)')\n",
    "# we only focus on class_0 and class_1 for the purpose of simplicity. (do binary classification)\n",
    "X = data.data[0:130]\n",
    "y = data.target[0:130]\n",
    "\n",
    "X_train_tmp, X_test_tmp, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# Scale the data since we will be using gradient ascent\n",
    "scaler = preprocessing.StandardScaler().fit(X_train_tmp)\n",
    "X_train = scaler.transform(X_train_tmp)\n",
    "X_test = scaler.transform(X_test_tmp)\n",
    "\n",
    "# Append a column of ones to X_train and X_test\n",
    "ones = np.ones(X_train.shape[0]).reshape((X_train.shape[0],1))\n",
    "X_train = np.hstack((ones, X_train))\n",
    "ones = np.ones(X_test.shape[0]).reshape((X_test.shape[0], 1))\n",
    "X_test = np.hstack((ones, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "[ 0.20979497 -0.55540613 -0.03690661 -0.19429156  0.37258714 -0.22065127\n",
      "  0.         -0.16862806  0.05294112 -0.05000681 -0.37839111  0.\n",
      " -0.15395089 -0.72279016]\n",
      "Accuracy: 0.9767441860465116\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.9\n",
    "new_w = coordinate_descent_lasso(X_train, y_train, learning_rate,num_iters=5000, lamda=0.02)\n",
    "print(f'Weights:\\n{new_w}')\n",
    "# set threshold\n",
    "threshold = 0.5\n",
    "y_pred = hypothesis(X_test,new_w)\n",
    "ones_idx,zeros_idx = np.where(y_pred >=threshold), np.where(y_pred <threshold)\n",
    "y_pred[ones_idx],y_pred[zeros_idx] = 1, 0\n",
    "# change y_pred shape to the same shape as y_pred\n",
    "y_pred = y_pred.reshape(y_pred.shape[0])\n",
    "print('Accuracy: {}'.format(np.where(y_pred == y_test)[0].size / y_pred.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "Lasso shrinks the less important featureâ€™s coefficient to zero thus, removing some feature altogether. So Lasso is ideal to select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
