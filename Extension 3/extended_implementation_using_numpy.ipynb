{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Zhihao Zhang\n",
    "# NetID: zz2432\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy.random as r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My implementation on softmax activation on outer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    return np.exp(z) / float(sum(np.exp(z)))\n",
    "\n",
    "def softmax_deriv(z):\n",
    "    return softmax(z) * (1 - softmax(z))\n",
    "\n",
    "def relu(z):\n",
    "    output = np.vstack((np.zeros(z.size), z)).T\n",
    "    return np.max(output, axis=1)\n",
    "\n",
    "def relu_deriv(z):\n",
    "    # slope= 0 if z is negative, slope = 1 if z is positive\n",
    "    temp = np.zeros(z.size)\n",
    "    zero_idx,one_idx = np.where(z < 0),np.where(z >= 0)\n",
    "    temp[zero_idx],temp[one_idx] = 0, 1\n",
    "    return temp\n",
    "\n",
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    return -(y-a_out) * softmax_deriv(z_out) \n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * relu_deriv(z_l)\n",
    "\n",
    "def feed_forward(x, W, b):\n",
    "    a = {1: x} # create a dictionary for holding the a values for all levels\n",
    "    z = { } # create a dictionary for holding the z values for all the layers\n",
    "    for l in range(1, len(W) + 1): # for each layer\n",
    "        node_in = a[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l]\n",
    "        # if this is last layer, apply softmax\n",
    "        if l == len(W):\n",
    "            a[l+1] = softmax(z[l+1])\n",
    "        else:   \n",
    "            a[l+1] = relu(z[l+1]) \n",
    "    return a, z\n",
    "\n",
    "def train_nn(nn_structure, X, y, iter_num=1000, alpha=0.25, lamb = 0.001):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%100 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(N):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            a, z = feed_forward(X[i, :], W, b)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-a[l]))\n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            # add regularization term\n",
    "            W[l] += -alpha * (1.0/N * tri_W[l] + lamb * W[l])\n",
    "            b[l] += -alpha * (1.0/N * tri_b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/N * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y\n",
    "\n",
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} #creating a dictionary i.e. a set of key: value pairs\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1])) #Return “continuous uniform” random floats in the half-open interval [0.0, 1.0). \n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b\n",
    "\n",
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b\n",
    "\n",
    "def convert_y_to_vect(y, n_labels):\n",
    "    y_vect = np.zeros((len(y), n_labels))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset used in class: load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits # The MNIST data set is in scikit learn data set\n",
    "digits=load_digits()\n",
    "X_scale = preprocessing.StandardScaler()  # It is important in neural networks to scale the data\n",
    "X = X_scale.fit_transform(digits.data)\n",
    "y = digits.target\n",
    "#Split the data into training and test set.  70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# one-hot encoding y_train\n",
    "n_labels = np.unique(y_train).size\n",
    "y_train = convert_y_to_vect(y_train, n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 64\n",
      "Output size: 10\n"
     ]
    }
   ],
   "source": [
    "print(f'Input size: {X_train.shape[1]}\\nOutput size: {n_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 1000 iterations\n",
      "Iteration 0 of 1000\n",
      "Iteration 100 of 1000\n",
      "Iteration 200 of 1000\n",
      "Iteration 300 of 1000\n",
      "Iteration 400 of 1000\n",
      "Iteration 500 of 1000\n",
      "Iteration 600 of 1000\n",
      "Iteration 700 of 1000\n",
      "Iteration 800 of 1000\n",
      "Iteration 900 of 1000\n",
      "CPU times: user 2min 17s, sys: 390 ms, total: 2min 17s\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nn_structure = [64, 30, 10]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_train, iter_num=1000, alpha=0.25, lamb = 0.001)\n",
    "# get the prediction accuracy and print\n",
    "y_pred = predict_y(W, b, X_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "res = np.where(y_pred==y_test)[0].size /y_test.size\n",
    "print(f'Accuracy: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset outside class:  fetch_covtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "cover_type = fetch_covtype()\n",
    "X_scale = preprocessing.StandardScaler()  # It is important in neural networks to scale the data\n",
    "X = X_scale.fit_transform(cover_type.data)\n",
    "y = cover_type.target\n",
    "#Split the data into training and test set.  70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# since our label is from 1 to 7: for convenience, subtract all label by 1 to match the indices of one-hot encoding\n",
    "y_train = y_train - 1\n",
    "# one-hot encoding y_train\n",
    "n_labels = np.unique(y_train).size\n",
    "y_train = convert_y_to_vect(y_train, n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sample to Train: 406708\n",
      "Input size: 54\n",
      "Output size: 7\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Sample to Train: {X_train.shape[0]}\\nInput size: {X_train.shape[1]}\\nOutput size: {n_labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only try 100 iterations: The naive NN algorithm took so long to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 100 iterations\n",
      "Iteration 0 of 100\n",
      "Iteration 10 of 100\n",
      "Iteration 20 of 100\n",
      "Iteration 30 of 100\n",
      "Iteration 40 of 100\n",
      "Iteration 50 of 100\n",
      "Iteration 60 of 100\n",
      "Iteration 70 of 100\n",
      "Iteration 80 of 100\n",
      "Iteration 90 of 100\n",
      "CPU times: user 1h 12min 46s, sys: 11.2 s, total: 1h 12min 58s\n",
      "Wall time: 1h 13min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nn_structure = [54, 30, 7]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_train, iter_num=100, alpha=0.25, lamb = 0.001)\n",
    "# get the prediction accuracy and print\n",
    "y_pred = predict_y(W, b, X_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5528960895906003\n"
     ]
    }
   ],
   "source": [
    "# get back the orignial label\n",
    "y_pred = y_pred + 1\n",
    "res = np.where(y_pred==y_test)[0].size /y_test.size\n",
    "print(f'Accuracy: {res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
